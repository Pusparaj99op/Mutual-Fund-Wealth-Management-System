{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3e8683",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Engineering\n",
    "\n",
    "This notebook demonstrates loading raw NAV CSVs, basic cleaning, feature engineering (rolling stats, returns, volatility), saving per-scheme features, and a simple train/validation workflow suitable for time-series forecasting and recommendation tasks.\n",
    "\n",
    "Sections:\n",
    "1. Imports & setup\n",
    "2. Load processed dataset (or build from raw)\n",
    "3. Quick data validation\n",
    "4. EDA\n",
    "5. Feature engineering & sklearn pipeline\n",
    "6. Train/validation split\n",
    "7. Baseline model training\n",
    "8. Evaluation & metrics\n",
    "9. Hyperparameter tuning\n",
    "10. Persist artifacts (joblib)\n",
    "11. Diagnostics & visualizations\n",
    "12. Tests and reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Import libraries and environment setup\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('..').resolve()\n",
    "DATA_RAW = ROOT / 'data' / 'raw' / 'csv'\n",
    "DATA_PROCESSED = ROOT / 'data' / 'processed'\n",
    "FEATURES_DIR = ROOT / 'data' / 'features'\n",
    "CACHE_DIR = ROOT / 'data' / 'cache' / 'joblib'\n",
    "PLOTS_DIR = ROOT / 'reports' / 'plots'\n",
    "\n",
    "for p in [DATA_PROCESSED, FEATURES_DIR, CACHE_DIR, PLOTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger('mf-preproc')\n",
    "logger.info('Environment set up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Load processed dataset (or build from raw CSVs)\n",
    "\n",
    "def load_processed_dataset():\n",
    "    # Try loading a single processed file first\n",
    "    candidates = list(DATA_PROCESSED.glob('*.parquet')) + list(DATA_PROCESSED.glob('*.csv'))\n",
    "    if candidates:\n",
    "        logger.info(f'Found processed files: {len(candidates)}. Loading and concatenating')\n",
    "        dfs = [pd.read_parquet(p) if p.suffix=='.parquet' else pd.read_csv(p, parse_dates=['date']) for p in candidates]\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        logger.info(f'Loaded processed dataset with shape {df.shape}')\n",
    "        return df\n",
    "    # Fallback: build from raw CSVs\n",
    "    raw_files = sorted(DATA_RAW.glob('nav_*.csv'))\n",
    "    logger.info(f'Found {len(raw_files)} raw CSV files')\n",
    "    records = []\n",
    "    for f in tqdm(raw_files[:50], desc='Loading sample raw CSVs'):\n",
    "        try:\n",
    "            tmp = pd.read_csv(f, parse_dates=['date'])\n",
    "            tmp['scheme_code'] = f.stem.split('_')[-1]\n",
    "            records.append(tmp)\n",
    "        except Exception as e:\n",
    "            logger.warning(f'Failed to read {f}: {e}')\n",
    "    if not records:\n",
    "        raise FileNotFoundError('No raw data found in data/raw/csv. Ensure files are downloaded.')\n",
    "    df = pd.concat(records, ignore_index=True)\n",
    "    df = df.rename(columns=lambda s: s.strip().lower())\n",
    "    logger.info(f'Built dataset from raw CSVs: {df.shape}')\n",
    "    return df\n",
    "\n",
    "# Load into memory (small sample for the notebook run)\n",
    "df = load_processed_dataset()\n",
    "# Ensure date is sorted within each scheme\n",
    "df = df.sort_values(['scheme_code','date']).reset_index(drop=True)\n",
    "\n",
    "# Show a peek\n",
    "print(df.head())\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec68ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Quick data validation and schema checks\n",
    "\n",
    "def validate_nav_df(df: pd.DataFrame):\n",
    "    assert 'date' in df.columns, \"'date' column missing\"\n",
    "    assert 'nav' in df.columns, \"'nav' column missing\"\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "        raise ValueError('`date` must be datetime64')\n",
    "    if df['date'].isnull().any():\n",
    "        raise ValueError('Null dates found')\n",
    "    if df['nav'].isnull().all():\n",
    "        raise ValueError('All NAVs are null')\n",
    "    # simple range checks\n",
    "    if (df['nav']<=0).any():\n",
    "        logger.warning('NAV contains non-positive values; review data cleaning strategy')\n",
    "    logger.info('Validation passed (basic checks)')\n",
    "\n",
    "validate_nav_df(df)\n",
    "\n",
    "# Show date range per scheme (sample)\n",
    "print(df.groupby('scheme_code')['date'].agg(['min','max','count']).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acf291",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Exploratory data analysis (targeted)\n",
    "\n",
    "# Example: distribution of NAV values for a single sample scheme\n",
    "sample_scheme = df['scheme_code'].unique()[0]\n",
    "sub = df[df['scheme_code']==sample_scheme].copy()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.lineplot(data=sub, x='date', y='nav')\n",
    "plt.title(f'NAV over time - scheme {sample_scheme}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / f'nav_time_{sample_scheme}.png')\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(sub['nav'].describe())\n",
    "\n",
    "# Correlation between numeric features if present\n",
    "numeric_cols = sub.select_dtypes('number').columns.tolist()\n",
    "if len(numeric_cols)>1:\n",
    "    corr = sub[numeric_cols].corr()\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f')\n",
    "    plt.title('Correlation (sample)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / f'corr_{sample_scheme}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Feature engineering and preprocessing pipeline\n",
    "\n",
    "# Rolling features and returns\n",
    "\n",
    "def make_rolling_features(df: pd.DataFrame, windows=[7,30,90]):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values('date')\n",
    "    for w in windows:\n",
    "        df[f'roll_mean_{w}'] = df['nav'].rolling(w, min_periods=1).mean()\n",
    "        df[f'roll_std_{w}'] = df['nav'].rolling(w, min_periods=1).std().fillna(0)\n",
    "    df['ret_1d'] = df['nav'].pct_change().fillna(0)\n",
    "    df['ret_7d'] = df['nav'].pct_change(7).fillna(0)\n",
    "    df['log_nav'] = np.log1p(df['nav'])\n",
    "    return df\n",
    "\n",
    "# Create features for all schemes and save per-scheme features\n",
    "for code, g in tqdm(df.groupby('scheme_code'), desc='Feature generation'):\n",
    "    fg = make_rolling_features(g)\n",
    "    out = FEATURES_DIR / f'features_{code}.parquet'\n",
    "    fg.to_parquet(out)\n",
    "\n",
    "print('Feature files saved to', FEATURES_DIR)\n",
    "\n",
    "# Simple sklearn pipeline for numerical features\n",
    "num_features = [c for c in df.columns if c.startswith('roll_') or c.startswith('ret_') or c=='log_nav']\n",
    "cat_features = ['scheme_code']\n",
    "\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, num_features),\n",
    "    ('cat', cat_transformer, cat_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3398e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Train/validation split and CV setup\n",
    "\n",
    "# For forecasting, create a simple supervised learning dataset: predict next-day NAV (or n-day ahead)\n",
    "HORIZON = 7  # days ahead to predict\n",
    "\n",
    "def make_supervised(df: pd.DataFrame, horizon=HORIZON):\n",
    "    df = df.sort_values('date')\n",
    "    df['target_nav'] = df['nav'].shift(-horizon)\n",
    "    # drop rows where target is na (end of series)\n",
    "    out = df.dropna(subset=['target_nav']).copy()\n",
    "    out['target_ret'] = out['target_nav'] / out['nav'] - 1\n",
    "    return out\n",
    "\n",
    "# Build a small train/test split for demonstration\n",
    "sample_code = df['scheme_code'].unique()[0]\n",
    "sample_df = make_supervised(df[df['scheme_code']==sample_code])\n",
    "\n",
    "train_size = int(len(sample_df)*0.8)\n",
    "train_df = sample_df.iloc[:train_size]\n",
    "test_df = sample_df.iloc[train_size:]\n",
    "\n",
    "# TimeSeriesSplit for CV\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print('Train/Test sizes:', len(train_df), len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00495188",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Baseline model training\n",
    "\n",
    "# Prepare feature matrix and target\n",
    "X_train = train_df[num_features + cat_features]\n",
    "y_train = train_df['target_ret']\n",
    "X_test = test_df[num_features + cat_features]\n",
    "y_test = test_df['target_ret']\n",
    "\n",
    "# Fit preprocessing and a simple model pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('pre', preprocessor),\n",
    "    ('est', RandomForestRegressor(n_estimators=100, random_state=SEED, n_jobs=-1))\n",
    "])\n",
    "\n",
    "logger.info('Fitting baseline RandomForest model')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, CACHE_DIR / 'rf_baseline.joblib')\n",
    "print('Saved baseline model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0262de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Model evaluation and metrics\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return dict(mae=mae, rmse=rmse, r2=r2)\n",
    "\n",
    "metrics = regression_metrics(y_test, y_pred)\n",
    "print(metrics)\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel('Actual target_ret')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Actual (test)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'pred_vs_actual.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Hyperparameter tuning and cross-validation\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'est__n_estimators': [50,100,200],\n",
    "    'est__max_depth': [5,10,20,None]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(model, param_distributions, n_iter=6, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=2, random_state=SEED)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "logger.info(f'Best params: {search.best_params_}')\n",
    "best_model = search.best_estimator_\n",
    "joblib.dump(best_model, CACHE_DIR / 'rf_best.joblib')\n",
    "print('Saved tuned model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abf31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Persist models and artifacts (joblib)\n",
    "\n",
    "# Example: load and quick inference check\n",
    "loaded = joblib.load(CACHE_DIR / 'rf_best.joblib')\n",
    "sample_X = X_test.iloc[:5]\n",
    "print('Sample inference:', loaded.predict(sample_X))\n",
    "\n",
    "# Save the preprocessing pipeline explicitly\n",
    "joblib.dump(preprocessor, CACHE_DIR / 'preprocessor.joblib')\n",
    "print('Saved preprocessor and model artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bdcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Visualization of results and diagnostic plots\n",
    "\n",
    "# Feature importances (if model supports)\n",
    "if hasattr(loaded.named_steps['est'], 'feature_importances_'):\n",
    "    fi = loaded.named_steps['est'].feature_importances_\n",
    "    # retrieve feature names after preprocessor\n",
    "    try:\n",
    "        num_names = num_features\n",
    "        cat_names = list(loaded.named_steps['pre'].transformers_[1][1].named_steps['ohe'].get_feature_names_out(cat_features))\n",
    "        feat_names = num_names + list(cat_names)\n",
    "    except Exception:\n",
    "        feat_names = [f'feat_{i}' for i in range(len(fi))]\n",
    "    imp_df = pd.DataFrame({'feature': feat_names, 'importance': fi}).sort_values('importance', ascending=False).head(30)\n",
    "    plt.figure(figsize=(6,8))\n",
    "    sns.barplot(data=imp_df, x='importance', y='feature')\n",
    "    plt.title('Top feature importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'feature_importances.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Model does not expose feature_importances_')\n",
    "\n",
    "# Learning curve placeholder: use sklearn.learning_curve if desired (omitted for brevity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12. Notebook tests and reproducibility (seeds, logging)\n",
    "\n",
    "# Basic assertions\n",
    "assert SEED == 42\n",
    "assert (CACHE_DIR.exists())\n",
    "assert num_features, 'No numeric features found in dataset'\n",
    "\n",
    "# Small deterministic check\n",
    "x = np.random.RandomState(SEED).rand(3)\n",
    "print('Deterministic sample:', x)\n",
    "\n",
    "logger.info('Notebook complete. Next: extend to multi-scheme training, add advanced models (LSTM/Transformer), and build evaluation/backtesting scripts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a3517-fd56-471c-bf45-15f5de8c85ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
